{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, time, gym, logging, pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as tf_layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, date\n",
    "from copy import copy\n",
    "\n",
    "from gym.spaces import MultiDiscrete\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from functools import partial\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.common import tf_util, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.vec_env import VecEnv, VecFrameStack\n",
    "from stable_baselines.common.vec_env.base_vec_env import VecEnvWrapper\n",
    "from stable_baselines.common.schedules import LinearSchedule\n",
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.policies import BasePolicy, nature_cnn, register_policy\n",
    "\n",
    "from stable_baselines.deepq.build_graph import build_act, build_act_with_param_noise\n",
    "from stable_baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from stable_baselines.deepq.policies import DQNPolicy, CnnPolicy\n",
    "\n",
    "from stable_baselines.a2c.utils import total_episode_reward_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ[\"KMP_AFFINITY\"] = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global use_wrapper\n",
    "global custom_reward\n",
    "global blocks_count\n",
    "global results_list\n",
    "global given_rewards\n",
    "\n",
    "use_wrapper = True\n",
    "custom_reward = 0.0\n",
    "\n",
    "blocks_count = 0\n",
    "results_list = []\n",
    "given_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_globals():\n",
    "    global blocks_count\n",
    "    global results_list\n",
    "    global given_rewards\n",
    "    \n",
    "    blocks_count = 0\n",
    "    results_list = []\n",
    "    given_rewards = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardWrapper(VecEnvWrapper):\n",
    "    def reset(self):\n",
    "        obs = self.venv.reset()\n",
    "        self.stackedobs[...] = 0\n",
    "        self.stackedobs[..., -obs.shape[-1]:] = obs\n",
    "        return self.stackedobs\n",
    "\n",
    "    def step_wait(self):\n",
    "        global blocks_count\n",
    "        global custom_reward\n",
    "        global given_rewards\n",
    "        global results_list\n",
    "        \n",
    "        observations, rewards, dones, infos = self.venv.step_wait()\n",
    "\n",
    "        if rewards[0] >= 1:\n",
    "            blocks_count += 1\n",
    "            given_rewards += 1\n",
    "            \n",
    "#         if custom_reward is not None:\n",
    "#             rewards[0] = custom_reward          \n",
    "#         if rewards[0] >= 1:\n",
    "#             given_rewards += 1\n",
    "\n",
    "        results_list.append(\n",
    "            {\n",
    "                'timestamp': '{0}'.format(datetime.now()),\n",
    "                'blocks': blocks_count,\n",
    "                'number of rewards': given_rewards\n",
    "            }\n",
    "        )\n",
    "            \n",
    "        return observations, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDQN(DQN):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyDQN, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def setup_model(self):\n",
    "\n",
    "        with SetVerbosity(self.verbose):\n",
    "            assert not isinstance(self.action_space, gym.spaces.Box), \\\n",
    "                \"Error: DQN cannot output a gym.spaces.Box action space.\"\n",
    "\n",
    "            if isinstance(self.policy, partial):\n",
    "                test_policy = self.policy.func\n",
    "            else:\n",
    "                test_policy = self.policy\n",
    "            assert issubclass(test_policy, DQNPolicy), \"Error: the input policy for the DQN model must be \" \\\n",
    "                                                       \"an instance of DQNPolicy.\"\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.sess = tf_util.make_session(graph=self.graph)\n",
    "\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "                self.act, self._train_step, self.update_target, self.step_model = my_build_train(\n",
    "                    q_func=partial(self.policy, **self.policy_kwargs),\n",
    "                    ob_space=self.observation_space,\n",
    "                    ac_space=self.action_space,\n",
    "                    optimizer=optimizer,\n",
    "                    gamma=self.gamma,\n",
    "                    grad_norm_clipping=10,\n",
    "                    param_noise=self.param_noise,\n",
    "                    sess=self.sess,\n",
    "                    full_tensorboard_log=self.full_tensorboard_log,\n",
    "                    double_q=self.double_q\n",
    "                )\n",
    "                self.proba_step = self.step_model.proba_step\n",
    "                self.params = tf_util.get_trainable_vars(\"deepq\")\n",
    "\n",
    "                tf_util.initialize(self.sess)\n",
    "                self.update_target(sess=self.sess)\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "    def learn(self, total_timesteps, callback=None, seed=None, log_interval=100, tb_log_name=\"DQN\",\n",
    "              reset_num_timesteps=True, replay_wrapper=None):\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn(seed)\n",
    "\n",
    "            if self.prioritized_replay:\n",
    "                self.replay_buffer = PrioritizedReplayBuffer(self.buffer_size, alpha=self.prioritized_replay_alpha)\n",
    "                if self.prioritized_replay_beta_iters is None:\n",
    "                    prioritized_replay_beta_iters = total_timesteps\n",
    "                else:\n",
    "                    prioritized_replay_beta_iters = self.prioritized_replay_beta_iters\n",
    "                self.beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "                                                    initial_p=self.prioritized_replay_beta0,\n",
    "                                                    final_p=1.0)\n",
    "            else:\n",
    "                self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "                self.beta_schedule = None\n",
    "\n",
    "            if replay_wrapper is not None:\n",
    "                assert not self.prioritized_replay, \"Prioritized replay buffer is not supported by HER\"\n",
    "                self.replay_buffer = replay_wrapper(self.replay_buffer)\n",
    "\n",
    "            self.exploration = LinearSchedule(schedule_timesteps=int(self.exploration_fraction * total_timesteps),\n",
    "                                              initial_p=1.0, final_p=self.exploration_final_eps)\n",
    "\n",
    "            episode_rewards = [0.0]\n",
    "            episode_successes = []\n",
    "            obs = self.env.reset()\n",
    "            reset = True\n",
    "            self.save(writer.get_logdir() + '/init_model.h5')\n",
    "            self.episode_reward = np.zeros((1,))\n",
    "            timesteps_last_log = 0\n",
    "            avr_ep_len_per_log = None\n",
    "            \n",
    "            global results_list\n",
    "\n",
    "            for _ in trange(total_timesteps, desc='Learning model'):\n",
    "                if callback is not None:\n",
    "                    if callback(locals(), globals()) is False:\n",
    "                        break\n",
    "                kwargs = {}\n",
    "                if not self.param_noise:\n",
    "                    update_eps = self.exploration.value(self.num_timesteps)\n",
    "                    update_param_noise_threshold = 0.\n",
    "                else:\n",
    "                    update_eps = 0.\n",
    "                    update_param_noise_threshold = \\\n",
    "                        -np.log(1. - self.exploration.value(self.num_timesteps) +\n",
    "                                self.exploration.value(self.num_timesteps) / float(self.env.action_space.n))\n",
    "                    kwargs['reset'] = reset\n",
    "                    kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                    kwargs['update_param_noise_scale'] = True\n",
    "                with self.sess.as_default():\n",
    "                    action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
    "                env_action = action\n",
    "                reset = False\n",
    "                new_obs, rew, done, info = self.env.step(env_action)\n",
    "                self.replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "                obs = new_obs\n",
    "\n",
    "                if writer is not None:\n",
    "                    ep_rew = np.array([rew]).reshape((1, -1))\n",
    "                    ep_done = np.array([done]).reshape((1, -1))\n",
    "                    self.episode_reward = total_episode_reward_logger(self.episode_reward, ep_rew, ep_done, writer,\n",
    "                                                                      self.num_timesteps)\n",
    "\n",
    "                episode_rewards[-1] += rew\n",
    "                if done:\n",
    "                    maybe_is_success = info.get('is_success')\n",
    "                    if maybe_is_success is not None:\n",
    "                        episode_successes.append(float(maybe_is_success))\n",
    "                    if not isinstance(self.env, VecEnv):\n",
    "                        obs = self.env.reset()\n",
    "                    episode_rewards.append(0.0)\n",
    "                    reset = True\n",
    "\n",
    "                can_sample = self.replay_buffer.can_sample(self.batch_size)\n",
    "\n",
    "                if can_sample and self.num_timesteps > self.learning_starts \\\n",
    "                        and self.num_timesteps % self.train_freq == 0:\n",
    "                    if self.prioritized_replay:\n",
    "                        experience = self.replay_buffer.sample(self.batch_size,\n",
    "                                                               beta=self.beta_schedule.value(self.num_timesteps))\n",
    "                        (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "                    else:\n",
    "                        obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size)\n",
    "                        weights, batch_idxes = np.ones_like(rewards), None\n",
    "\n",
    "                    if writer is not None:\n",
    "                        if (1 + self.num_timesteps) % 100 == 0:\n",
    "                            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                            run_metadata = tf.RunMetadata()\n",
    "                            summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                                  dones, weights, sess=self.sess, options=run_options,\n",
    "                                                                  run_metadata=run_metadata)\n",
    "                            writer.add_run_metadata(run_metadata, 'step%d' % self.num_timesteps)\n",
    "                        else:\n",
    "                            summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                                  dones, weights, sess=self.sess)\n",
    "                        writer.add_summary(summary, self.num_timesteps)\n",
    "                    else:\n",
    "                        _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n",
    "                                                        sess=self.sess)\n",
    "\n",
    "                    if self.prioritized_replay:\n",
    "                        new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                        self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "                if can_sample and self.num_timesteps > self.learning_starts and \\\n",
    "                        self.num_timesteps % self.target_network_update_freq == 0:\n",
    "                    self.update_target(sess=self.sess)\n",
    "\n",
    "                if len(episode_rewards[-101:-1]) == 0:\n",
    "                    mean_100ep_reward = -np.inf\n",
    "                else:\n",
    "                    mean_100ep_reward = round(float(np.mean(episode_rewards[-101:-1])), 1)\n",
    "\n",
    "                if len(episode_rewards) % log_interval == 0:\n",
    "                    avr_ep_len_per_log = (self.num_timesteps - timesteps_last_log) / log_interval\n",
    "                    timesteps_last_log = self.num_timesteps\n",
    "\n",
    "                num_episodes = len(episode_rewards)\n",
    "                if self.verbose >= 1 and done and log_interval is not None and len(episode_rewards) % log_interval == 0:\n",
    "                    logger.record_tabular(\"steps\", self.num_timesteps)\n",
    "                    logger.record_tabular(\"episodes\", num_episodes)\n",
    "                    if len(episode_successes) > 0:\n",
    "                        logger.logkv(\"success rate\", np.mean(episode_successes[-100:]))\n",
    "                    logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "                    logger.record_tabular(\"% time spent exploring\",\n",
    "                                          int(100 * self.exploration.value(self.num_timesteps)))\n",
    "                    logger.record_tabular(\"avr length of last logged ep\", avr_ep_len_per_log)\n",
    "                    logger.dump_tabular()\n",
    "\n",
    "                self.num_timesteps += 1\n",
    "                \n",
    "            self.save(writer.get_logdir() + '/final_model.h5')\n",
    "            \n",
    "            results = pd.DataFrame(results_list).drop_duplicates()\n",
    "            results.to_csv(writer.get_logdir() + '/results.csv', index=False)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def evaluate(self, n_episodes=2):\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "        id = 'BreakoutNoFrameskip-v4'\n",
    "        num_env = 1\n",
    "        n_stack = 4\n",
    "        left_lives = 5\n",
    "        seed = 0\n",
    "        episodes = 0\n",
    "        score = 0\n",
    "        frames = 0\n",
    "        frames_per_episode = list()\n",
    "        scores = [list() for i in range(n_episodes)]\n",
    "\n",
    "        env = make_atari_env(id, num_env=num_env, seed=seed)\n",
    "        env = VecFrameStack(env, n_stack=n_stack)\n",
    "        obs = env.reset()\n",
    "\n",
    "        while (n_episodes - episodes) > 0:\n",
    "            frames += 1\n",
    "            action, _states = self.predict(obs)\n",
    "            obs, rewards, dones, info = env.step(action)\n",
    "            env.render()\n",
    "            score += rewards[0]\n",
    "            if dones:\n",
    "                logging.debug('You died')\n",
    "                logging.debug(f'Score = {score}')\n",
    "                scores[episodes].append(score)\n",
    "                score = 0\n",
    "                left_lives -= 1\n",
    "            if not left_lives:\n",
    "                logging.debug('Episode ended')\n",
    "                logging.info(f'Scores per life: {scores[episodes]}')\n",
    "                frames_per_episode.append(frames)\n",
    "                frames = 0\n",
    "                episodes += 1\n",
    "                left_lives = 5\n",
    "\n",
    "        s = list(map(sum, scores))\n",
    "        avg_s = int(sum(s) / len(s))\n",
    "        avg_f = int(sum(frames_per_episode) / len(frames_per_episode))\n",
    "\n",
    "        logging.info(f'Played {n_episodes} episodes')\n",
    "        logging.info(f'Scores per episode : {s}')\n",
    "        logging.info(f'Average score per episode : {avg_s}')\n",
    "        logging.info(f'Average number of frames per episode : {avg_f}')\n",
    "\n",
    "        return avg_f, avg_s\n",
    "\n",
    "def my_build_train(q_func, ob_space, ac_space, optimizer, sess, grad_norm_clipping=None,\n",
    "                   gamma=1.0, double_q=True, scope=\"deepq\", reuse=None,\n",
    "                   param_noise=False, param_noise_filter_func=None, full_tensorboard_log=False):\n",
    "    n_actions = ac_space.nvec if isinstance(ac_space, MultiDiscrete) else ac_space.n\n",
    "    with tf.variable_scope(\"input\", reuse=reuse):\n",
    "        stochastic_ph = tf.placeholder(tf.bool, (), name=\"stochastic\")\n",
    "        update_eps_ph = tf.placeholder(tf.float32, (), name=\"update_eps\")\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if param_noise:\n",
    "            act_f, obs_phs = build_act_with_param_noise(q_func, ob_space, ac_space, stochastic_ph, update_eps_ph, sess,\n",
    "                                                        param_noise_filter_func=param_noise_filter_func)\n",
    "        else:\n",
    "            act_f, obs_phs = build_act(q_func, ob_space, ac_space, stochastic_ph, update_eps_ph, sess)\n",
    "\n",
    "        with tf.variable_scope(\"step_model\", reuse=True, custom_getter=tf_util.outer_scope_getter(\"step_model\")):\n",
    "            step_model = q_func(sess, ob_space, ac_space, 1, 1, None, reuse=True, obs_phs=obs_phs)\n",
    "        q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + \"/model\")\n",
    "        my_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                           scope=tf.get_variable_scope().name + \"/model/action_value/fully_connected_1\")\n",
    "\n",
    "        with tf.variable_scope(\"target_q_func\", reuse=False):\n",
    "            target_policy = q_func(sess, ob_space, ac_space, 1, 1, None, reuse=False)\n",
    "        target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                               scope=tf.get_variable_scope().name + \"/target_q_func\")\n",
    "\n",
    "        double_q_values = None\n",
    "        double_obs_ph = target_policy.obs_ph\n",
    "        if double_q:\n",
    "            with tf.variable_scope(\"double_q\", reuse=True, custom_getter=tf_util.outer_scope_getter(\"double_q\")):\n",
    "                double_policy = q_func(sess, ob_space, ac_space, 1, 1, None, reuse=True)\n",
    "                double_q_values = double_policy.q_values\n",
    "                double_obs_ph = double_policy.obs_ph\n",
    "\n",
    "    with tf.variable_scope(\"loss\", reuse=reuse):\n",
    "        act_t_ph = tf.placeholder(tf.int32, [None], name=\"action\")\n",
    "        rew_t_ph = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "        done_mask_ph = tf.placeholder(tf.float32, [None], name=\"done\")\n",
    "        importance_weights_ph = tf.placeholder(tf.float32, [None], name=\"weight\")\n",
    "\n",
    "        q_t_selected = tf.reduce_sum(step_model.q_values * tf.one_hot(act_t_ph, n_actions), axis=1)\n",
    "\n",
    "        if double_q:\n",
    "            q_tp1_best_using_online_net = tf.argmax(double_q_values, axis=1)\n",
    "            q_tp1_best = tf.reduce_sum(target_policy.q_values * tf.one_hot(q_tp1_best_using_online_net, n_actions),\n",
    "                                       axis=1)\n",
    "        else:\n",
    "            q_tp1_best = tf.reduce_max(target_policy.q_values, axis=1)\n",
    "        q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best\n",
    "\n",
    "        q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n",
    "\n",
    "        td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)\n",
    "        errors = tf_util.huber_loss(td_error)\n",
    "        weighted_error = tf.reduce_mean(importance_weights_ph * errors)\n",
    "\n",
    "        tf.summary.scalar(\"td_error\", tf.reduce_mean(td_error))\n",
    "        tf.summary.scalar(\"loss\", weighted_error)\n",
    "\n",
    "        if full_tensorboard_log:\n",
    "            tf.summary.histogram(\"td_error\", td_error)\n",
    "\n",
    "        update_target_expr = []\n",
    "        for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name),\n",
    "                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "            update_target_expr.append(var_target.assign(var))\n",
    "        update_target_expr = tf.group(*update_target_expr)\n",
    "\n",
    "        print('Trainable tensors:')\n",
    "        for v in my_q_func_vars:\n",
    "            print(v)\n",
    "        gradients = optimizer.compute_gradients(weighted_error, var_list=my_q_func_vars)\n",
    "        if grad_norm_clipping is not None:\n",
    "            for i, (grad, var) in enumerate(gradients):\n",
    "                if grad is not None:\n",
    "                    gradients[i] = (tf.clip_by_norm(grad, grad_norm_clipping), var)\n",
    "\n",
    "    with tf.variable_scope(\"input_info\", reuse=False):\n",
    "        tf.summary.scalar('rewards', tf.reduce_mean(rew_t_ph))\n",
    "        tf.summary.scalar('importance_weights', tf.reduce_mean(importance_weights_ph))\n",
    "\n",
    "        if full_tensorboard_log:\n",
    "            tf.summary.histogram('rewards', rew_t_ph)\n",
    "            tf.summary.histogram('importance_weights', importance_weights_ph)\n",
    "            if tf_util.is_image(obs_phs[0]):\n",
    "                tf.summary.image('observation', obs_phs[0])\n",
    "            elif len(obs_phs[0].shape) == 1:\n",
    "                tf.summary.histogram('observation', obs_phs[0])\n",
    "\n",
    "    optimize_expr = optimizer.apply_gradients(gradients)\n",
    "\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    train = tf_util.function(\n",
    "        inputs=[\n",
    "            obs_phs[0],\n",
    "            act_t_ph,\n",
    "            rew_t_ph,\n",
    "            target_policy.obs_ph,\n",
    "            double_obs_ph,\n",
    "            done_mask_ph,\n",
    "            importance_weights_ph\n",
    "        ],\n",
    "        outputs=[summary, td_error],\n",
    "        updates=[optimize_expr]\n",
    "    )\n",
    "    update_target = tf_util.function([], [], updates=[update_target_expr])\n",
    "\n",
    "    return act_f, train, update_target, step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFeedForwardPolicy(DQNPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None,\n",
    "                 cnn_extractor=nature_cnn, feature_extraction=\"cnn\",\n",
    "                 obs_phs=None, layer_norm=False, dueling=True, act_fun=tf.nn.relu, **kwargs):\n",
    "        super(MyFeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps,\n",
    "                                                n_batch, dueling=dueling, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"), obs_phs=obs_phs)\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is None:\n",
    "            layers = [64, 64]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            with tf.variable_scope(\"action_value\"):\n",
    "                if feature_extraction == \"cnn\":\n",
    "                    extracted_features = cnn_extractor(self.processed_obs, **kwargs)\n",
    "                    action_out = extracted_features\n",
    "                else:\n",
    "                    extracted_features = tf.layers.flatten(self.processed_obs)\n",
    "                    action_out = extracted_features\n",
    "                    for layer_size in layers:\n",
    "                        action_out = tf_layers.fully_connected(action_out, num_outputs=layer_size, activation_fn=None)\n",
    "                        if layer_norm:\n",
    "                            action_out = tf_layers.layer_norm(action_out, center=True, scale=True)\n",
    "                        action_out = act_fun(action_out)\n",
    "\n",
    "                action_scores = tf_layers.fully_connected(action_out, num_outputs=self.n_actions, activation_fn=None)\n",
    "                action_scores = tf_layers.fully_connected(action_scores, num_outputs=self.n_actions, activation_fn=None)\n",
    "\n",
    "            if self.dueling:\n",
    "                with tf.variable_scope(\"state_value\"):\n",
    "                    state_out = extracted_features\n",
    "                    for layer_size in layers:\n",
    "                        state_out = tf_layers.fully_connected(state_out, num_outputs=layer_size, activation_fn=None)\n",
    "                        if layer_norm:\n",
    "                            state_out = tf_layers.layer_norm(state_out, center=True, scale=True)\n",
    "                        state_out = act_fun(state_out)\n",
    "                    state_score = tf_layers.fully_connected(state_out, num_outputs=1, activation_fn=None)\n",
    "                action_scores_mean = tf.reduce_mean(action_scores, axis=1)\n",
    "                action_scores_centered = action_scores - tf.expand_dims(action_scores_mean, axis=1)\n",
    "                q_out = state_score + action_scores_centered\n",
    "            else:\n",
    "                q_out = action_scores\n",
    "\n",
    "        self.q_values = q_out\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=True):\n",
    "        q_values, actions_proba = self.sess.run([self.q_values, self.policy_proba], {self.obs_ph: obs})\n",
    "        if deterministic:\n",
    "            actions = np.argmax(q_values, axis=1)\n",
    "        else:\n",
    "            actions = np.zeros((len(obs),), dtype=np.int64)\n",
    "            for action_idx in range(len(obs)):\n",
    "                actions[action_idx] = np.random.choice(self.n_actions, p=actions_proba[action_idx])\n",
    "\n",
    "        return actions, q_values, None\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "\n",
    "class MyCnnPolicy(MyFeedForwardPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                 reuse=False, obs_phs=None, dueling=True, **_kwargs):\n",
    "        super(MyCnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"cnn\", obs_phs=obs_phs, dueling=dueling,\n",
    "                                        layer_norm=False, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_env():\n",
    "    global use_wrapper\n",
    "\n",
    "    env = make_atari_env('BreakoutNoFrameskip-v4', num_env=1, seed=0)\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    if use_wrapper:\n",
    "        return RewardWrapper(env)\n",
    "    else:\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env):\n",
    "    file = open('App/models/BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "    _, zoo_weights = pickle.load(file)\n",
    "\n",
    "    model = MyDQN(MyCnnPolicy, env, double_q=False, learning_starts=1, learning_rate=0.000005,\n",
    "                  tensorboard_log='tensor/', verbose=2, exploration_fraction=0.0002,\n",
    "                  prioritized_replay=True, exploration_final_eps=0.00002)\n",
    "\n",
    "    zoo_model = DQN(CnnPolicy, env, double_q=False, learning_starts=0.000005)\n",
    "    zoo_model.load_parameters(zoo_weights)\n",
    "\n",
    "    model.load_parameters(zoo_model.get_parameters(), exact_match=False)\n",
    "    params = model.get_parameters()\n",
    "    r = (np.random.rand(4, 4) - 0.5) * 0.15\n",
    "    params['deepq/model/action_value/fully_connected_1/biases:0'] = np.zeros(4)\n",
    "    params['deepq/model/action_value/fully_connected_1/weights:0'] = np.identity(4) + r\n",
    "    model.load_parameters(params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable tensors:\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/weights:0' shape=(4, 4) dtype=float32_ref>\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/biases:0' shape=(4,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "env = build_env()\n",
    "original_model = build_model(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a792c196a1a4c9eb7d20ff9d2bcdb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cea5f4fb15e4cf18633bc76ba9e10aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccfd5cc42fa42ad8e56783eb4fa2975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 0        |\n",
      "| avr length of last l... | 67.4     |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 1.1      |\n",
      "| steps                   | 6741     |\n",
      "--------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278dbf21f9244e95a0677d0cca3684b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc92bb03c39452c9b3730ffc47b895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d12ae716524ad6ac75f573e769750e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6535cf1bece74713828d5126f47c2688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72453626208242fe87b4110d91c25416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Learning model', max=18000.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(8):\n",
    "    reset_globals()\n",
    "    model = copy(original_model)\n",
    "    model.learn(18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
