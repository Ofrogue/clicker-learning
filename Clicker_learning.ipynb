{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Humans train dogs by delivering rewards to specific actions so that the dog will associate certain actions and situations to either positive (to be repeated again) or negative (to be avoided) values. Similarly, artificial agents are now trained automatically by reinforcement signals delivered after a goal has been achieved by the agent. Unfortunately, this process is very slow and requires many samples to learn. The aim of this project is to test whether it is possible to train a character in a video-game by delivering the reward (mouse click) at any event of teacher's choice (not only at the end of a goal). What would the best strategy for delivering a limited number of rewards (reward-shaping)? How does it compare to state-of-the-art reinforcement learning algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "We are using:\n",
    "* DQN algorithm\n",
    "* gym emviroment\n",
    "* stable baselines https://github.com/hill-a/stable-baselines\n",
    "* with pretrained models from zoo https://github.com/araffin/rl-baselines-zoo\n",
    "* gym enviroment https://github.com/openai/gym\n",
    "* atari game Breakout\n",
    "\n",
    "To conduct the experiment we will:\n",
    "* adopt a state-of-the-art model from the zoo\n",
    "* freeze all convolutional layers (or freeze all layers except the last $n$)\n",
    "* add noise to the remained layers\n",
    "* develop a customized reward mechanism based on a human reaction\n",
    "  + run and render the enviroment\n",
    "  + recieve a reward from user's clicks\n",
    "  + update the unfrozen layers according to the reward\n",
    "* run experiments with a human teacher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements & Installation\n",
    "We suggest using conda  \n",
    "```conda create -n clickerlearning```  \n",
    "```conda activate clickerlearning```  \n",
    "\n",
    "```conda install python==3.7 --yes && conda install -c conda-forge tensorflow --yes && conda install opencv --yes && conda install jupyter --yes && pip install gym==0.11.0 gym[atari] stable-baselines keyboard```  \n",
    "stable-baselines are slightly outdated according to the latest changes in gym. Thus we use an older version of gym.  \n",
    "You would need to run from root to use ```keyboard```  \n",
    "run ```sudo [path to your required enviroment]/bin/jupyter notebook --allow-root ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.cmd_util import make_atari_env # rl-zoo model is custom in contrast to gym defaults\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy, CnnPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env.base_vec_env import VecEnvWrapper\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "import threading, time\n",
    "import keyboard\n",
    "\n",
    "global_reward = 0.0\n",
    "\n",
    "def reward_checker():\n",
    "    global global_reward\n",
    "    while True:\n",
    "        keyboard.wait('space')       \n",
    "        global_reward = 1.0\n",
    "        time.sleep(0.05)\n",
    "\n",
    "threading.Thread(target=reward_checker).start()\n",
    "\n",
    "        \n",
    "class VecRewardWrapper(VecEnvWrapper):\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all environments\n",
    "        \"\"\"\n",
    "        obs = self.venv.reset()\n",
    "        self.stackedobs[...] = 0\n",
    "        self.stackedobs[..., -obs.shape[-1]:] = obs\n",
    "        return self.stackedobs\n",
    "    \n",
    "    def step_wait(self):\n",
    "        global global_reward\n",
    "        observations, rewards, dones, infos = self.venv.step_wait()\n",
    "        print(rewards)\n",
    "        rewards[0] = global_reward\n",
    "        # reward can be modified here\n",
    "        global_reward = 0.0\n",
    "        return observations, rewards, dones, infos\n",
    "\n",
    "\n",
    "env_id = 'BreakoutNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, num_env=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecRewardWrapper(env)\n",
    "\n",
    "\n",
    "model = DQN(CnnPolicy, env, verbose=2)\n",
    "\n",
    "file = open('BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "model_dict, model_weights = pickle.load(file)\n",
    "model.load_parameters(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# oserve our agent in action\n",
    "# probably notebook is not the best enviroment now\n",
    "# reward is actually collected from listener global_reward that waits for a space to be pressed\n",
    "\n",
    "n_frames = 1000\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(n_frames):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    print(\"Action:\", action, \"Reward:\", rewards, \"Done:\", dones)\n",
    "    time.sleep(0.1)\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'deepq/eps:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/c1/w:0' shape=(8, 8, 4, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/c1/b:0' shape=(1, 32, 1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/c2/w:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/c2/b:0' shape=(1, 64, 1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/c3/w:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/c3/b:0' shape=(1, 64, 1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/fc1/w:0' shape=(3136, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/fc1/b:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/fully_connected/weights:0' shape=(512, 4) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/action_value/fully_connected/biases:0' shape=(4,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/state_value/fully_connected/weights:0' shape=(512, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/state_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/state_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/state_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/state_value/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/model/state_value/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/c1/w:0' shape=(8, 8, 4, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/c1/b:0' shape=(1, 32, 1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/c2/w:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/c2/b:0' shape=(1, 64, 1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/c3/w:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/c3/b:0' shape=(1, 64, 1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/fc1/w:0' shape=(3136, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/fc1/b:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected/weights:0' shape=(512, 4) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/action_value/fully_connected/biases:0' shape=(4,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected/weights:0' shape=(512, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'deepq/target_q_func/model/state_value/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='deepq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating TensorBoard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(CnnPolicy, env, verbose=2)\n",
    "\n",
    "file = open('BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "model_dict, model_weights = pickle.load(file)\n",
    "model.load_parameters(model_weights)\n",
    "\n",
    "with model.sess as sess:\n",
    "    writer = tf.summary.FileWriter(\"tensor_files\", sess.graph)\n",
    "    sess.run(model.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='deepq')[-1])\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating files, you should have a directory in your project called ```tensor_files```. In order to load ```tensor_files``` run the following command ```tensorboard --logdir=tensor_files``` in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
