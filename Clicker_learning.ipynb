{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Humans train dogs by delivering rewards to specific actions so that the dog will associate certain actions and situations to either positive (to be repeated again) or negative (to be avoided) values. Similarly, artificial agents are now trained automatically by reinforcement signals delivered after a goal has been achieved by the agent. Unfortunately, this process is very slow and requires many samples to learn. The aim of this project is to test whether it is possible to train a character in a video-game by delivering the reward (mouse click) at any event of teacher's choice (not only at the end of a goal). What would the best strategy for delivering a limited number of rewards (reward-shaping)? How does it compare to state-of-the-art reinforcement learning algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "We are using:\n",
    "* DQN algorithm\n",
    "* gym emviroment\n",
    "* stable baselines https://github.com/hill-a/stable-baselines\n",
    "* with pretrained models from zoo https://github.com/araffin/rl-baselines-zoo\n",
    "* gym enviroment https://github.com/openai/gym\n",
    "* atari game Breakout\n",
    "\n",
    "To conduct the experiment we will:\n",
    "* adopt a state-of-the-art model from the zoo\n",
    "* freeze all convolutional layers (or freeze all layers except the last $n$)\n",
    "* add noise to the remained layers\n",
    "* develop a customized reward mechanism based on a human reaction\n",
    "  + run and render the enviroment\n",
    "  + recieve a reward from user's clicks\n",
    "  + update the unfrozen layers according to the reward\n",
    "* run experiments with a human teacher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements & Installation\n",
    "We suggest using conda  \n",
    "```conda create -n clickerlearning```  \n",
    "```conda activate clickerlearning```  \n",
    "\n",
    "```conda install python==3.7 --yes && conda install -c conda-forge tensorflow --yes && conda install opencv --yes && conda install jupyter --yes && pip install gym==0.11.0 gym[atari] stable-baselines==2.8.0 keyboard```  \n",
    "stable-baselines are slightly outdated according to the latest changes in gym. Thus we use an older version of gym.  \n",
    "You would need to run from root to use ```keyboard```  \n",
    "run ```sudo [path to your required enviroment]/bin/jupyter notebook --allow-root ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.cmd_util import make_atari_env # rl-zoo model is custom in contrast to gym defaults\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy, CnnPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "import pickle\n",
    "\n",
    "from myCnnPolicy import MyCnnPolicy\n",
    "from myDQN import MyDQN\n",
    "\n",
    "env_id = 'BreakoutNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, num_env=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env.base_vec_env import VecEnvWrapper\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "import threading, time\n",
    "import keyboard\n",
    "\n",
    "global_reward = 0.0\n",
    "\n",
    "def reward_checker():\n",
    "    global global_reward\n",
    "    while True:\n",
    "        keyboard.wait('space')       \n",
    "        global_reward = 1.0\n",
    "        time.sleep(0.05)\n",
    "\n",
    "threading.Thread(target=reward_checker).start()\n",
    "\n",
    "        \n",
    "class VecRewardWrapper(VecEnvWrapper):\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all environments\n",
    "        \"\"\"\n",
    "        obs = self.venv.reset()\n",
    "        self.stackedobs[...] = 0\n",
    "        self.stackedobs[..., -obs.shape[-1]:] = obs\n",
    "        return self.stackedobs\n",
    "    \n",
    "    def step_wait(self):\n",
    "        global global_reward\n",
    "        observations, rewards, dones, infos = self.venv.step_wait()\n",
    "        print(rewards)\n",
    "        rewards[0] = global_reward\n",
    "        # reward can be modified here\n",
    "        global_reward = 0.0\n",
    "        return observations, rewards, dones, infos\n",
    "\n",
    "\n",
    "env_id = 'BreakoutNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, num_env=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecRewardWrapper(env)\n",
    "\n",
    "\n",
    "model = DQN(CnnPolicy, env, verbose=2)\n",
    "\n",
    "file = open('BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "model_dict, model_weights = pickle.load(file)\n",
    "model.load_parameters(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kir/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Trainable tensors:\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/weights:0' shape=(4, 4) dtype=float32_ref>\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/biases:0' shape=(4,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From /home/kir/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from myCnnPolicy import MyCnnPolicy\n",
    "\n",
    "env_id = 'BreakoutNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, num_env=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "# custop MyCnnPolicy ads an extra linear layer (4,4) for an action_value scope\n",
    "model = MyDQN(MyCnnPolicy, env, verbose=2, double_q=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zoo_model():\n",
    "    model = DQN(CnnPolicy, env, double_q=False, learning_starts=10, tensorboard_log='./tensor_files/',verbose=2)\n",
    "    file = open('BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "    model_dict, model_weights = pickle.load(file)\n",
    "    model.load_parameters(model_weights)\n",
    "    return model\n",
    "\n",
    "def get_zoo_parameters():\n",
    "    \"\"\"loads all available params from BreakoutNoFrameskip-v4.pkl \n",
    "    privided by https://github.com/araffin/rl-baselines-zoo\n",
    "    \"\"\"\n",
    "    model = load_zoo_model()\n",
    "    return model.get_parameters()\n",
    "\n",
    "def load_custom_model(*args, **kwargs):\n",
    "    model = MyDQN(MyCnnPolicy, env, *args, **kwargs)\n",
    "    model.load_parameters(get_zoo_parameters(), exact_match=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# we altered our model with an extra layer\n",
    "# new tensors will be initialized randomly with a default initialization\n",
    "#model = load_custom_model(double_q=False, learning_starts=1000, tensorboard_log='./tensor_files/', verbose=2)\n",
    "#model = load_zoo_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the added layer is an identity matrix, than we get the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.859064134522201"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "a = model.get_parameters()['deepq/model/action_value/fully_connected_1/weights:0']\n",
    "u = np.identity(4)\n",
    "linalg.norm(a-u*a)\n",
    "# frobenius norm of not diagonality of matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable tensors:\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/weights:0' shape=(4, 4) dtype=float32_ref>\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/biases:0' shape=(4,) dtype=float32_ref>\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 67       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 3286     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 22       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 0.6      |\n",
      "| steps                   | 7906     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 5.5      |\n",
      "| steps                   | 27852    |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = load_custom_model(double_q=False, \n",
    "                          learning_starts=1000, \n",
    "                          tensorboard_log='./tensor_files/', \n",
    "                          verbose=2,\n",
    "                          prioritized_replay=True\n",
    "                         )\n",
    "model.learn(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic double_q=False, learning_starts=1000,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 3317     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 6924     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 10259    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 0.4      |\n",
      "| steps                   | 14012    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 0.4      |\n",
      "| steps                   | 17791    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 78       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 0.6      |\n",
      "| steps                   | 22413    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 0.6      |\n",
      "| steps                   | 27293    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 69       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 0.5      |\n",
      "| steps                   | 31608    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 37304    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 57       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 43286    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 49593    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 43       |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 57542    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 1.9      |\n",
      "| steps                   | 67072    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 21       |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 79659    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a2a566b634c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, seed, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m    256\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                             summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n\u001b[0;32m--> 258\u001b[0;31m                                                                   dones, weights, sess=self.sess)\n\u001b[0m\u001b[1;32m    259\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/stable_baselines/common/tf_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_model = model.learn(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kir/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Trainable tensors:\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/weights:0' shape=(4, 4) dtype=float32_ref>\n",
      "<tf.Variable 'deepq/model/action_value/fully_connected_1/biases:0' shape=(4,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From /home/kir/miniconda3/envs/clickerlearning/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 69       |\n",
      "| avr length of last l... | 31.1     |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 0.2      |\n",
      "| steps                   | 3112     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 36       |\n",
      "| avr length of last l... | 33.3     |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 6467     |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = load_custom_model(double_q=False, \n",
    "                          learning_starts=1000, \n",
    "                          verbose=2,\n",
    "                         )\n",
    "\n",
    "model.learn(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [1] Reward: [0.] Done: [False]\n",
      "fin\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# oserve our agent in action\n",
    "# probably notebook is not the best enviroment now\n",
    "# reward is actually collected from listener global_reward that waits for a space to be pressed\n",
    "\n",
    "# with a custom randomized policy our model does not give any suffisient behaviour \n",
    "# that what we need: a broken agent that would recover with our human reward\n",
    "\n",
    "#%matplotlib notebook\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "import time\n",
    "n_frames = 100\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(n_frames):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    ax.imshow(env.venv.render(mode='rgb_array'))\n",
    "    display(fig)\n",
    "    clear_output(wait=True)\n",
    "    # print(\"Action:\", action, \"Reward:\", rewards, \"Done:\", dones)\n",
    "    time.sleep(0.01)\n",
    "env.close()\n",
    "print('fin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating TensorBoard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(CnnPolicy, env, verbose=2, double_q=True)\n",
    "\n",
    "file = open('BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "model_dict, model_weights = pickle.load(file)\n",
    "model.load_parameters(model_weights)\n",
    "\n",
    "with model.sess as sess:\n",
    "    writer = tf.summary.FileWriter(\"tensor_files\", sess.graph)\n",
    "    sess.run(model.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='deepq')[-1])\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating files, you should have a directory in your project called ```tensor_files```. In order to load ```tensor_files``` run the following command ```tensorboard --logdir=tensor_files``` in the same directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
