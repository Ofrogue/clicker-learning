{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Humans train dogs by delivering rewards to specific actions so that the dog will associate certain actions and situations to either positive (to be repeated again) or negative (to be avoided) values. Similarly, artificial agents are now trained automatically by reinforcement signals delivered after a goal has been achieved by the agent. Unfortunately, this process is very slow and requires many samples to learn. The aim of this project is to test whether it is possible to train a character in a video-game by delivering the reward (mouse click) at any event of teacher's choice (not only at the end of a goal). What would the best strategy for delivering a limited number of rewards (reward-shaping)? How does it compare to state-of-the-art reinforcement learning algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "We are using:\n",
    "* DQN algorithm\n",
    "* gym emviroment\n",
    "* stable baselines https://github.com/hill-a/stable-baselines\n",
    "* with pretrained models from zoo https://github.com/araffin/rl-baselines-zoo\n",
    "* gym enviroment https://github.com/openai/gym\n",
    "* atari game Breakout\n",
    "\n",
    "To conduct the experiment we will:\n",
    "* adopt a state-of-the-art model from the zoo\n",
    "* freeze all convolutional layers (or freeze all layers except the last $n$)\n",
    "* add noise to the remained layers\n",
    "* develop a customized reward mechanism based on a human reaction\n",
    "  + run and render the enviroment\n",
    "  + recieve a reward from user's clicks\n",
    "  + update the unfrozen layers according to the reward\n",
    "* run experiments with a human teacher\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements & Installation\n",
    "We suggest using conda  \n",
    "```conda create -n clickerlearning```  \n",
    "```conda activate clickerlearning```  \n",
    "\n",
    "```conda install python==3.7 --yes && conda install -c conda-forge tensorflow --yes && conda install opencv --yes && pip install gym==0.11.0 stable-baselines```  \n",
    "stable-baselines are slightly outdated according to the latest changes in gym. Thus we use an older version of gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from stable_baselines.common.cmd_util import make_atari_env # rl-zoo model is custom in contrast to gym defaults\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy, CnnPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kir/miniconda3/envs/stabas/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kir/miniconda3/envs/stabas/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "env_id = 'BreakoutNoFrameskip-v4'\n",
    "env = make_atari_env(env_id, num_env=1, seed=0)\n",
    "# Frame-stacking with 4 frames to fit the pretrained zoo configuration\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "model = DQN(CnnPolicy, env, verbose=1)\n",
    "\n",
    "file = open('BreakoutNoFrameskip-v4.pkl', 'rb')\n",
    "model_dict, model_weights = pickle.load(file)\n",
    "\n",
    "model.load_parameters(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oserve our agent in action\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "n_frames = 1000\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(n_frames):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    print(\"Action:\", action, \"Reward:\", rewards, \"Done:\", dones)\n",
    "    time.sleep(0.01)\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
